{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/snli_train.tsv\",sep=\"\\t\")\n",
    "df_v = pd.read_csv(\"data/snli_val.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>891</td>\n",
       "      <td>999</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>A group of numbered participants walk down the...</td>\n",
       "      <td>People are outside .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "count                                                1000   \n",
       "unique                                                891   \n",
       "top     A group of numbered participants walk down the...   \n",
       "freq                                                    2   \n",
       "\n",
       "                   sentence2    label  \n",
       "count                   1000     1000  \n",
       "unique                   999        3  \n",
       "top     People are outside .  neutral  \n",
       "freq                       2      338  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Three women on a stage , one wearing red shoes...</td>\n",
       "      <td>There are two women standing on the stage</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Four people sit on a subway two read books , o...</td>\n",
       "      <td>Multiple people are on a subway together , wit...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bicycles stationed while a group of people soc...</td>\n",
       "      <td>People get together near a stand of bicycles .</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Man in overalls with two horses .</td>\n",
       "      <td>a man in overalls with two horses</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Man observes a wavelength given off by an elec...</td>\n",
       "      <td>The man is examining what wavelength is given ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  Three women on a stage , one wearing red shoes...   \n",
       "1  Four people sit on a subway two read books , o...   \n",
       "2  bicycles stationed while a group of people soc...   \n",
       "3                  Man in overalls with two horses .   \n",
       "4  Man observes a wavelength given off by an elec...   \n",
       "\n",
       "                                           sentence2          label  \n",
       "0          There are two women standing on the stage  contradiction  \n",
       "1  Multiple people are on a subway together , wit...     entailment  \n",
       "2     People get together near a stand of bicycles .     entailment  \n",
       "3                  a man in overalls with two horses     entailment  \n",
       "4  The man is examining what wavelength is given ...     entailment  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = df[\"sentence1\"].values#[0:5000]\n",
    "second = df[\"sentence2\"].values#[0:5000]\n",
    "labels = df[\"label\"].values#[0:5000]\n",
    "\n",
    "first_v = df_v[\"sentence1\"].values#[0:500]\n",
    "second_v = df_v[\"sentence2\"].values#[0:500]\n",
    "labels_v = df_v[\"label\"].values#[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two gentlemen in tuxes play the keyboard and guitar . The two guys are in a band . neutral\n"
     ]
    }
   ],
   "source": [
    "print(first[-1],second[-1],labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'entailment', 'neutral', 'contradiction',\n",
       "       'contradiction', 'entailment', 'contradiction', 'neutral',\n",
       "       'entailment', 'entailment'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {\n",
    "    'entailment':0,\n",
    "    'contradiction':1,\n",
    "    'neutral':2\n",
    "}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = label_to_idx[labels[i]]\n",
    "    \n",
    "for i in range(len(labels_v)):\n",
    "    labels_v[i] = label_to_idx[labels_v[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(\"int32\")\n",
    "labels_v = labels_v.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "  #tokens = tokenizer(sent)\n",
    "  return [word.lower() for word in sent.split(\" \") if (word not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helldoifjsdoihfs', 'numpy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"helldoifjsdoihfs . numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, ..., 2, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset1,dataset2):\n",
    "    token_dataset1 = []\n",
    "    token_dataset2 = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset1:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset1.append(tokens)\n",
    "        all_tokens += tokens\n",
    "        \n",
    "    for sample in dataset2:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset2.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset1,token_dataset2, all_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0,\n",
       "        1, 2, 1, 0, 0, 2, 2, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 0, 0, 2, 2, 0, 0,\n",
       "        2, 1, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 2, 0, 1, 2, 2,\n",
       "        0, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 0, 2, 1, 2, 0, 0, 1, 0, 1, 2, 1, 2, 1,\n",
       "        1, 0, 2, 1, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 2, 2, 0, 1, 2, 1, 2, 1, 2, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 2, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        2, 1, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        2, 1, 1, 1, 2, 0, 2, 1, 0, 1, 0, 2, 1, 0, 2, 0, 1, 2, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 0, 2, 0, 1, 1, 2, 0, 2, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 2, 1, 0, 2, 2, 0, 0, 2, 1, 1, 2,\n",
       "        1, 0, 0, 2, 2, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 2, 2, 0,\n",
       "        1, 1, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 0, 2, 0, 1, 2, 1, 1, 1, 1, 2, 0, 0,\n",
       "        1, 0, 2, 1, 1, 2, 2, 2, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2, 2, 1, 1, 2, 0, 0,\n",
       "        2, 0, 0, 2, 1, 2, 2, 1, 1, 1, 0, 2, 1, 1, 2, 2, 0, 1, 2, 0, 2, 0, 2, 0,\n",
       "        1, 2, 1, 1, 0, 2, 2, 0, 0, 2, 2, 0, 1, 0, 0, 2, 1, 2, 2, 2, 2, 2, 0, 0,\n",
       "        1, 2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 2, 0, 2, 2, 2, 1, 0, 0, 0, 0,\n",
       "        1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 0, 1, 0, 2, 0, 0, 1, 1, 2, 1,\n",
       "        2, 2, 2, 2, 2, 2, 1, 1, 2, 0, 0, 1, 1, 2, 1, 2, 0, 1, 2, 0, 1, 2, 2, 2,\n",
       "        1, 1, 0, 2, 1, 2, 2, 1, 2, 1, 2, 0, 0, 2, 1, 2, 1, 0, 0, 2, 1, 1, 2, 1,\n",
       "        2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 0,\n",
       "        2, 2, 1, 2, 2, 1, 1, 0, 0, 0, 2, 2, 2, 1, 2, 0, 2, 2, 2, 0, 0, 0, 2, 1,\n",
       "        1, 1, 0, 0, 1, 1, 2, 2, 1, 2, 2, 2, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 1, 0, 2, 0, 1, 2, 2, 1, 0, 2,\n",
       "        2, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 1, 0, 2, 2, 0, 0, 2, 0, 0, 1, 2, 1, 1,\n",
       "        2, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 0, 2,\n",
       "        2, 2, 0, 2, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 0, 2, 2, 2, 1, 0, 2, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 2, 0, 2, 1, 0, 2, 2, 0, 2, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2,\n",
       "        1, 2, 2, 1, 2, 1, 1, 1, 1, 0, 2, 2, 0, 2, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0,\n",
       "        1, 1, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 1, 2, 1, 0, 1, 2, 1, 1,\n",
       "        1, 1, 1, 2, 1, 0, 0, 1, 0, 0, 2, 1, 2, 2, 0, 1, 0, 2, 1, 2, 0, 0, 2, 1,\n",
       "        2, 2, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 2,\n",
       "        2, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2, 1, 0, 0, 1, 0, 2,\n",
       "        2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 2, 1,\n",
       "        0, 0, 2, 2, 0, 1, 0, 0, 1, 1, 2, 2, 1, 1, 2, 0, 1, 2, 0, 0, 1, 0, 2, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 1, 2, 1, 2, 0, 0,\n",
       "        1, 2, 2, 1, 1, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 1,\n",
       "        1, 2, 1, 0, 2, 0, 0, 2, 1, 2, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 0,\n",
       "        0, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 1, 0, 2, 2, 1, 1, 2, 2,\n",
       "        1, 2, 1, 2, 0, 2, 1, 2, 1, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 1, 2,\n",
       "        2, 0, 0, 2, 2, 2, 2, 2, 1, 1, 2, 0, 2, 0, 0, 2, 1, 0, 2, 1, 1, 1, 1, 1,\n",
       "        2, 1, 2, 0, 1, 0, 2, 0, 0, 1, 2, 1, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(labels)\n",
    "torch.LongTensor(labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1_tokens,sent2_tokens,all_train_tokens = tokenize_dataset(first,second)\n",
    "sent1_tokens_v,sent2_tokens_v,all_train_tokens_v = tokenize_dataset(first_v,second_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'dog', 'is', 'jumping', 'high']\n",
      "['a', 'young', 'girl', 'in', 'a', 'pink', 'shirt', 'sitting', 'on', 'a']\n",
      "21006\n"
     ]
    }
   ],
   "source": [
    "print(sent2_tokens[10])\n",
    "print(all_train_tokens[0:10])\n",
    "print(len(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "token2id_v, id2token_v = build_vocab(all_train_tokens_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2token_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 6336 ; token alaska\n",
      "Token alaska; token id 6336\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1 dataset size is 100000\n",
      "sent2 dataset size is 100000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "sent1_indices = token2index_dataset(sent1_tokens)\n",
    "sent2_indices = token2index_dataset(sent2_tokens)\n",
    "sent1_indices_v = token2index_dataset(sent1_tokens_v)\n",
    "sent2_indices_v = token2index_dataset(sent2_tokens_v)\n",
    "#test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"sent1 dataset size is {}\".format(len(sent1_indices)))\n",
    "print (\"sent2 dataset size is {}\".format(len(sent2_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(sent1_indices_v)))\n",
    "#print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 3698, 11, 13, 43, 96, 138]\n",
      "['a', 'man', 'squinting', 'with', 'two', 'women', 'behind', 'him']\n",
      "[13, 43, 9, 111, 2, 6, 180]\n",
      "['two', 'women', 'are', 'watching', 'a', 'man', 'work']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "rand_training_example = random.randint(0, len(first) - 1)\n",
    "print (sent1_indices[rand_training_example])\n",
    "print(sent1_tokens[rand_training_example])\n",
    "print (sent2_indices[rand_training_example])\n",
    "print(sent2_tokens[rand_training_example])\n",
    "print(labels[rand_training_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(max([len(s) for s in sent1_tokens]),max([len(s) for s in sent2_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list1,data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.data_list2) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]\n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1,len(token_idx1),token_idx2,len(token_idx2),label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SNLIDataset(sent1_indices,sent2_indices,labels)\n",
    "val_dataset = SNLIDataset(sent1_indices_v,sent2_indices_v,labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'boy', 'wearing', 'the', 'blue', 'hooded', 'top', 'is', 'holding', 'a', 'baby', 'goat', 'in', 'his', 'arms']\n",
      "['a', 'boy', 'ran', 'from', 'a', 'goat']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print(train_dataset[0])\n",
    "s = val_dataset[99][0]\n",
    "t = val_dataset[99][2]\n",
    "#print(s)\n",
    "print([id2token[i] for i in s])\n",
    "print([id2token[i] for i in t])\n",
    "print(val_dataset[99][-1])\n",
    "#print([id2token[i] for i in range(len(train_dataset[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [2, 22, 688, 184, 78, 2, 512, 10, 2329][80, 688, 204, 1516];\n",
      "y 0\n"
     ]
    }
   ],
   "source": [
    "print(\"x {}{};\\ny {}\".format(train_dataset[99][0], train_dataset[99][2],train_dataset[99][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNLI_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[-1])\n",
    "        length_list1.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "    for datum in batch:\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.LongTensor(length_list1),torch.from_numpy(np.array(data_list2)), torch.LongTensor(length_list2), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "for i, (data1, lengths1,data2, lengths2, labels) in enumerate(val_loader):\n",
    "    print(data1.shape)\n",
    "    #print(lengths1[1].item())\n",
    "    ##print(data2[1])\n",
    "    #print(lengths2[1].item())\n",
    "    #print(labels[1].item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        #self.concat = \n",
    "        self.linear1 = nn.Linear(emb_dim,64)\n",
    "        self.linear2 = nn.Linear(64,32)\n",
    "        self.linear3 = nn.Linear(32,3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self,data1,length1,data2,length2):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        # out1 out2?\n",
    "        \"\"\"\n",
    "        out1 = self.embed(data1)\n",
    "        out1 = torch.sum(out1, dim=1)\n",
    "        out1 /= length1.view(length1.size()[0],1).expand_as(out1).float()\n",
    "        \n",
    "        out2 = self.embed(data2)\n",
    "        out2 = torch.sum(out2, dim=1)\n",
    "        out2 /= length2.view(length2.size()[0],1).expand_as(out2).float()\n",
    "        \n",
    "        out = torch.cat((out1,out2),dim=0)\n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "        \"\"\"\n",
    "        \n",
    "        prem_embed = self.embed(data1) # 64xlen\n",
    "        prem_embed = torch.sum(prem_embed, dim=1) #64\n",
    "        prem_embed /= length1.view(length1.size()[0],1).expand_as(prem_embed).float()\n",
    "        hypo_embed = self.embed(data2)\n",
    "        hypo_embed = torch.sum(hypo_embed, dim=1)\n",
    "        hypo_embed /= length2.view(length2.size()[0],1).expand_as(hypo_embed).float()\n",
    "        \n",
    "        out = prem_embed * hypo_embed\n",
    "        out = self.linear1(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.linear3(out.float())\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear3.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290691\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2], Step: [51/3125], Validation Acc: 39.9\n",
      "Epoch: [1/2], Step: [101/3125], Validation Acc: 41.1\n",
      "Epoch: [1/2], Step: [151/3125], Validation Acc: 43.9\n",
      "Epoch: [1/2], Step: [201/3125], Validation Acc: 48.5\n",
      "Epoch: [1/2], Step: [251/3125], Validation Acc: 47.3\n",
      "Epoch: [1/2], Step: [301/3125], Validation Acc: 51.8\n",
      "Epoch: [1/2], Step: [351/3125], Validation Acc: 53.0\n",
      "Epoch: [1/2], Step: [401/3125], Validation Acc: 54.8\n",
      "Epoch: [1/2], Step: [451/3125], Validation Acc: 53.7\n",
      "Epoch: [1/2], Step: [501/3125], Validation Acc: 56.7\n",
      "Epoch: [1/2], Step: [551/3125], Validation Acc: 56.8\n",
      "Epoch: [1/2], Step: [601/3125], Validation Acc: 57.8\n",
      "Epoch: [1/2], Step: [651/3125], Validation Acc: 58.0\n",
      "Epoch: [1/2], Step: [701/3125], Validation Acc: 59.8\n",
      "Epoch: [1/2], Step: [751/3125], Validation Acc: 59.8\n",
      "Epoch: [1/2], Step: [801/3125], Validation Acc: 60.3\n",
      "Epoch: [1/2], Step: [851/3125], Validation Acc: 60.2\n",
      "Epoch: [1/2], Step: [901/3125], Validation Acc: 59.2\n",
      "Epoch: [1/2], Step: [951/3125], Validation Acc: 60.5\n",
      "Epoch: [1/2], Step: [1001/3125], Validation Acc: 59.9\n",
      "Epoch: [1/2], Step: [1051/3125], Validation Acc: 61.2\n",
      "Epoch: [1/2], Step: [1101/3125], Validation Acc: 60.5\n",
      "Epoch: [1/2], Step: [1151/3125], Validation Acc: 60.7\n",
      "Epoch: [1/2], Step: [1201/3125], Validation Acc: 60.8\n",
      "Epoch: [1/2], Step: [1251/3125], Validation Acc: 61.0\n",
      "Epoch: [1/2], Step: [1301/3125], Validation Acc: 60.6\n",
      "Epoch: [1/2], Step: [1351/3125], Validation Acc: 60.8\n",
      "Epoch: [1/2], Step: [1401/3125], Validation Acc: 64.0\n",
      "Epoch: [1/2], Step: [1451/3125], Validation Acc: 60.8\n",
      "Epoch: [1/2], Step: [1501/3125], Validation Acc: 62.5\n",
      "Epoch: [1/2], Step: [1551/3125], Validation Acc: 62.3\n",
      "Epoch: [1/2], Step: [1601/3125], Validation Acc: 62.6\n",
      "Epoch: [1/2], Step: [1651/3125], Validation Acc: 61.4\n",
      "Epoch: [1/2], Step: [1701/3125], Validation Acc: 64.6\n",
      "Epoch: [1/2], Step: [1751/3125], Validation Acc: 63.3\n",
      "Epoch: [1/2], Step: [1801/3125], Validation Acc: 63.7\n",
      "Epoch: [1/2], Step: [1851/3125], Validation Acc: 63.1\n",
      "Epoch: [1/2], Step: [1901/3125], Validation Acc: 62.5\n",
      "Epoch: [1/2], Step: [1951/3125], Validation Acc: 63.5\n",
      "Epoch: [1/2], Step: [2001/3125], Validation Acc: 64.5\n",
      "Epoch: [1/2], Step: [2051/3125], Validation Acc: 63.9\n",
      "Epoch: [1/2], Step: [2101/3125], Validation Acc: 63.9\n",
      "Epoch: [1/2], Step: [2151/3125], Validation Acc: 63.6\n",
      "Epoch: [1/2], Step: [2201/3125], Validation Acc: 65.3\n",
      "Epoch: [1/2], Step: [2251/3125], Validation Acc: 65.4\n",
      "Epoch: [1/2], Step: [2301/3125], Validation Acc: 65.2\n",
      "Epoch: [1/2], Step: [2351/3125], Validation Acc: 63.6\n",
      "Epoch: [1/2], Step: [2401/3125], Validation Acc: 66.1\n",
      "Epoch: [1/2], Step: [2451/3125], Validation Acc: 65.9\n",
      "Epoch: [1/2], Step: [2501/3125], Validation Acc: 65.1\n",
      "Epoch: [1/2], Step: [2551/3125], Validation Acc: 64.2\n",
      "Epoch: [1/2], Step: [2601/3125], Validation Acc: 65.5\n",
      "Epoch: [1/2], Step: [2651/3125], Validation Acc: 65.4\n",
      "Epoch: [1/2], Step: [2701/3125], Validation Acc: 65.7\n",
      "Epoch: [1/2], Step: [2751/3125], Validation Acc: 64.4\n",
      "Epoch: [1/2], Step: [2801/3125], Validation Acc: 65.5\n",
      "Epoch: [1/2], Step: [2851/3125], Validation Acc: 66.0\n",
      "Epoch: [1/2], Step: [2901/3125], Validation Acc: 64.1\n",
      "Epoch: [1/2], Step: [2951/3125], Validation Acc: 65.3\n",
      "Epoch: [1/2], Step: [3001/3125], Validation Acc: 64.5\n",
      "Epoch: [1/2], Step: [3051/3125], Validation Acc: 64.1\n",
      "Epoch: [1/2], Step: [3101/3125], Validation Acc: 66.4\n",
      "Epoch: [2/2], Step: [51/3125], Validation Acc: 65.8\n",
      "Epoch: [2/2], Step: [101/3125], Validation Acc: 65.7\n",
      "Epoch: [2/2], Step: [151/3125], Validation Acc: 66.4\n",
      "Epoch: [2/2], Step: [201/3125], Validation Acc: 65.9\n",
      "Epoch: [2/2], Step: [251/3125], Validation Acc: 66.3\n",
      "Epoch: [2/2], Step: [301/3125], Validation Acc: 66.3\n",
      "Epoch: [2/2], Step: [351/3125], Validation Acc: 65.6\n",
      "Epoch: [2/2], Step: [401/3125], Validation Acc: 66.8\n",
      "Epoch: [2/2], Step: [451/3125], Validation Acc: 66.5\n",
      "Epoch: [2/2], Step: [501/3125], Validation Acc: 67.0\n",
      "Epoch: [2/2], Step: [551/3125], Validation Acc: 66.8\n",
      "Epoch: [2/2], Step: [601/3125], Validation Acc: 65.0\n",
      "Epoch: [2/2], Step: [651/3125], Validation Acc: 65.2\n",
      "Epoch: [2/2], Step: [701/3125], Validation Acc: 66.0\n",
      "Epoch: [2/2], Step: [751/3125], Validation Acc: 65.8\n",
      "Epoch: [2/2], Step: [801/3125], Validation Acc: 66.4\n",
      "Epoch: [2/2], Step: [851/3125], Validation Acc: 65.9\n",
      "Epoch: [2/2], Step: [901/3125], Validation Acc: 65.2\n",
      "Epoch: [2/2], Step: [951/3125], Validation Acc: 66.5\n",
      "Epoch: [2/2], Step: [1001/3125], Validation Acc: 66.2\n",
      "Epoch: [2/2], Step: [1051/3125], Validation Acc: 65.2\n",
      "Epoch: [2/2], Step: [1101/3125], Validation Acc: 65.8\n",
      "Epoch: [2/2], Step: [1151/3125], Validation Acc: 66.9\n",
      "Epoch: [2/2], Step: [1201/3125], Validation Acc: 65.9\n",
      "Epoch: [2/2], Step: [1251/3125], Validation Acc: 66.2\n",
      "Epoch: [2/2], Step: [1301/3125], Validation Acc: 66.4\n",
      "Epoch: [2/2], Step: [1351/3125], Validation Acc: 64.9\n",
      "Epoch: [2/2], Step: [1401/3125], Validation Acc: 66.0\n",
      "Epoch: [2/2], Step: [1451/3125], Validation Acc: 65.7\n",
      "Epoch: [2/2], Step: [1501/3125], Validation Acc: 65.4\n",
      "Epoch: [2/2], Step: [1551/3125], Validation Acc: 66.1\n",
      "Epoch: [2/2], Step: [1601/3125], Validation Acc: 65.6\n",
      "Epoch: [2/2], Step: [1651/3125], Validation Acc: 64.8\n",
      "Epoch: [2/2], Step: [1701/3125], Validation Acc: 66.5\n",
      "Epoch: [2/2], Step: [1751/3125], Validation Acc: 65.9\n",
      "Epoch: [2/2], Step: [1801/3125], Validation Acc: 66.0\n",
      "Epoch: [2/2], Step: [1851/3125], Validation Acc: 65.8\n",
      "Epoch: [2/2], Step: [1901/3125], Validation Acc: 66.0\n",
      "Epoch: [2/2], Step: [1951/3125], Validation Acc: 65.6\n",
      "Epoch: [2/2], Step: [2001/3125], Validation Acc: 66.0\n",
      "Epoch: [2/2], Step: [2051/3125], Validation Acc: 65.4\n",
      "Epoch: [2/2], Step: [2101/3125], Validation Acc: 65.8\n",
      "Epoch: [2/2], Step: [2151/3125], Validation Acc: 67.1\n",
      "Epoch: [2/2], Step: [2201/3125], Validation Acc: 65.7\n",
      "Epoch: [2/2], Step: [2251/3125], Validation Acc: 65.8\n",
      "Epoch: [2/2], Step: [2301/3125], Validation Acc: 65.5\n",
      "Epoch: [2/2], Step: [2351/3125], Validation Acc: 66.3\n",
      "Epoch: [2/2], Step: [2401/3125], Validation Acc: 66.0\n",
      "Epoch: [2/2], Step: [2451/3125], Validation Acc: 66.2\n",
      "Epoch: [2/2], Step: [2501/3125], Validation Acc: 66.1\n",
      "Epoch: [2/2], Step: [2551/3125], Validation Acc: 67.0\n",
      "Epoch: [2/2], Step: [2601/3125], Validation Acc: 67.8\n",
      "Epoch: [2/2], Step: [2651/3125], Validation Acc: 66.8\n",
      "Epoch: [2/2], Step: [2701/3125], Validation Acc: 66.3\n",
      "Epoch: [2/2], Step: [2751/3125], Validation Acc: 67.3\n",
      "Epoch: [2/2], Step: [2801/3125], Validation Acc: 65.9\n",
      "Epoch: [2/2], Step: [2851/3125], Validation Acc: 66.5\n",
      "Epoch: [2/2], Step: [2901/3125], Validation Acc: 65.7\n",
      "Epoch: [2/2], Step: [2951/3125], Validation Acc: 65.8\n",
      "Epoch: [2/2], Step: [3001/3125], Validation Acc: 66.1\n",
      "Epoch: [2/2], Step: [3051/3125], Validation Acc: 65.7\n",
      "Epoch: [2/2], Step: [3101/3125], Validation Acc: 65.7\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2 # number epoch to train\n",
    "val_acc_graph = []\n",
    "train_loss_graph = []\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    sample = []\n",
    "    for data1, length1, data2, length2, labels in loader:\n",
    "        data_batch1, length_batch1, data_batch2, length_batch2, label_batch = data1, length1, data2, length2, labels\n",
    "        outputs = F.softmax(model(data_batch1, length_batch1,data_batch2,length_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        temp = predicted\n",
    "        sample.append([temp,labels])\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), sample\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data1, length1, data2, length2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch1, length_batch1,data_batch2, length_batch2, label_batch = data1, length1, data2, length2, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch1, length_batch1,data_batch2,length_batch2)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            #pass\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)[0]\n",
    "            val_acc_graph.append(val_acc)\n",
    "            train_loss_graph.append(loss.item())\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "               epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 2 epochs\n",
      "Train Acc 86.16\n",
      "Val Acc 66.3\n",
      "Number of parameters 1290691\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Train Acc {}\".format(test_model(train_loader, model)[0]))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)[0]))\n",
    "print(\"Number of parameters {}\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(train_loss_graph))\n",
    "plt.plot(x, train_loss_graph)\n",
    "plt.title(\"Training loss: DNN, elem-mul, 10000 words, 128d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(val_acc_graph))\n",
    "plt.plot(x, val_acc_graph)\n",
    "plt.title(\"Validation accuracy: DNN, ele-mul, 10000 words, 128d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveList(myList,filename):\n",
    "    # the filename should mention the extension 'npy'\n",
    "    np.save(filename,myList)\n",
    "    print(\"Saved successfully!\")\n",
    "train_loss_graph = np.asarray(train_loss_graph, dtype=np.float32)\n",
    "val_acc_graph = np.asarray(val_acc_graph, dtype=np.float32)\n",
    "saveList(train_loss_graph,\"figures/DNN_train_elemmul_10000_128.npy\")\n",
    "saveList(val_acc_graph,\"figures/DNN_val_elemmul_10000_128.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
